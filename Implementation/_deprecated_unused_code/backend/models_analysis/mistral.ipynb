{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-23T00:46:08.426742Z","iopub.status.busy":"2025-02-23T00:46:08.426445Z","iopub.status.idle":"2025-02-23T00:46:08.440731Z","shell.execute_reply":"2025-02-23T00:46:08.439403Z","shell.execute_reply.started":"2025-02-23T00:46:08.426720Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'streamlit'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4f65d593bfa3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"]}],"source":["import streamlit as st\n","import pandas as pd\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import matplotlib.pyplot as plt\n","import io\n","import kagglehub\n","from kagglehub import KaggleDatasetAdapter\n","\n","\n","from huggingface_hub import login\n","\n","\n","# Authenticate with Hugging Face\n","token = \"\" # Replace with your token\n","login(token)\n","\n","\n","\n","\n","# Install required libraries in Kaggle Notebook (if needed)\n","!pip install transformers torch streamlit kagglehub --quiet\n","\n","\n","import streamlit as st\n","import pandas as pd\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import matplotlib.pyplot as plt\n","import io\n","import kagglehub\n","from kagglehub import KaggleDatasetAdapter\n","from huggingface_hub import login\n","\n","# Install required libraries in Kaggle Notebook (if needed)\n","!pip install transformers torch streamlit kagglehub --quiet\n","\n","\n","# Load Mistral 7B Model & Tokenizer\n","@st.cache_resource\n","def load_mistral():\n","    model_name = \"mistralai/Mistral-7B-v0.1\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=True)\n","    return model, tokenizer\n","\n","model, tokenizer = load_mistral()\n","print(\"model loaded\")\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load Kaggle Dataset\n","dataset_path = \"nuskhan1/housing\"\n","file_path = \"housing.csv\"\n","data = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, dataset_path, file_path)\n","st.write(\"### Preview of Data\")\n","st.dataframe(data.head())\n","\n","# Query Processing\n","question = st.text_input(\"Ask a question about the data:\")\n","\n","def generate_response(prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    output = model.generate(**inputs, max_length=512)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response\n","\n","if question:\n","    if \"graph\" in question.lower() and data is not None:\n","        st.write(\"Generating graph...\")\n","        plt.figure(figsize=(10, 5))\n","        \n","        if \"sales\" in question.lower():\n","            if \"date\" in data.columns and \"sales\" in data.columns:\n","                data[\"date\"] = pd.to_datetime(data[\"date\"])\n","                data.sort_values(\"date\", inplace=True)\n","                plt.plot(data[\"date\"], data[\"sales\"], marker='o', linestyle='-')\n","                plt.xlabel(\"Date\")\n","                plt.ylabel(\"Sales\")\n","                plt.title(\"Sales Trend\")\n","                plt.xticks(rotation=45)\n","                buf = io.BytesIO()\n","                plt.savefig(buf, format='png')\n","                st.image(buf)\n","            else:\n","                st.error(\"Dataset must have 'date' and 'sales' columns.\")\n","        else:\n","            st.error(\"Graph generation is only supported for sales data.\")\n","    else:\n","        with st.spinner(\"Processing...\"):\n","            response = generate_response(question)\n","        st.write(\"### AI Response:\")\n","        st.write(response)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2025-02-22T20:00:55.316332Z","iopub.status.idle":"2025-02-22T20:00:55.316701Z","shell.execute_reply":"2025-02-22T20:00:55.316558Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-23T00:46:27.631731Z","iopub.status.busy":"2025-02-23T00:46:27.631422Z","iopub.status.idle":"2025-02-23T00:48:25.173761Z","shell.execute_reply":"2025-02-23T00:48:25.172858Z","shell.execute_reply.started":"2025-02-23T00:46:27.631705Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05218e823e47488a82256c2ee28c71e4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fd7583be927450b8a631d0e17d2bbd5","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd701dae04f240fb8a6edd194b840e4d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"789cecbba152489fb31e5679f6b4a785","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9bb53f99c234c7eb92ca9ae0e13a6c6","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c7cd957a04b4353842b24b62e226f6b","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a4bf74b3a2a412fa34bf9ced11413b2","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a66595821284322979486b2a289a187","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b02da6aa4c44abcaebe3a8bd82e80b8","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7cd800242d74a87893b45ea151b91f0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3e8cf3a225c47a0b751da545cd90e9f","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model loaded\n"]}],"source":["import pandas as pd\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import matplotlib.pyplot as plt\n","import io\n","import kagglehub\n","from kagglehub import KaggleDatasetAdapter\n","from huggingface_hub import login\n","\n","# Authenticate with Hugging Face\n","token = \"\"  # Replace with your Hugging Face token\n","login(token)\n","\n","# Install required libraries in Kaggle Notebook (if needed)\n","!pip install transformers torch kagglehub --quiet\n","\n","# Load Mistral 7B Model & Tokenizer\n","def load_mistral():\n","    model_name = \"mistralai/Mistral-7B-v0.1\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=True)\n","    return model, tokenizer\n","\n","model, tokenizer = load_mistral()\n","print(\"Model loaded\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","\n","# Load Kaggle Dataset\n","dataset_path = \"nuskhan1/housing\"\n","file_path = \"housing.csv\"\n","data = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, dataset_path, file_path)\n","print(\"Dataset loaded\")\n","print(data.head())\n","\n","# Query Processing (Manually define the question)\n","question = \"Generate a graph for sales data over time\"  # Manually input your question here\n","\n","def generate_response(prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")  # Use CPU instead of CUDA\n","    output = model.generate(**inputs, max_length=512)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response\n","\n","if question:\n","    if \"graph\" in question.lower() and data is not None:\n","        print(\"Generating graph...\")\n","        plt.figure(figsize=(10, 5))\n","        \n","        if \"sales\" in question.lower():\n","            if \"date\" in data.columns and \"sales\" in data.columns:\n","                data[\"date\"] = pd.to_datetime(data[\"date\"])\n","                data.sort_values(\"date\", inplace=True)\n","                plt.plot(data[\"date\"], data[\"sales\"], marker='o', linestyle='-')\n","                plt.xlabel(\"Date\")\n","                plt.ylabel(\"Sales\")\n","                plt.title(\"Sales Trend\")\n","                plt.xticks(rotation=45)\n","                buf = io.BytesIO()\n","                plt.savefig(buf, format='png')\n","                plt.show()  # Display the graph in the notebook\n","            else:\n","                print(\"Error: Dataset must have 'date' and 'sales' columns.\")\n","        else:\n","            print(\"Error: Graph generation is only supported for sales data.\")\n","    else:\n","        response = generate_response(question)\n","        print(\"AI Response:\")\n","        print(response)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-02-23T00:48:32.131365Z","iopub.status.busy":"2025-02-23T00:48:32.130735Z","iopub.status.idle":"2025-02-23T00:48:32.308233Z","shell.execute_reply":"2025-02-23T00:48:32.307486Z","shell.execute_reply.started":"2025-02-23T00:48:32.131331Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset loaded\n","   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0    -122.23     37.88                41.0        880.0           129.0   \n","1    -122.22     37.86                21.0       7099.0          1106.0   \n","2    -122.24     37.85                52.0       1467.0           190.0   \n","3    -122.25     37.85                52.0       1274.0           235.0   \n","4    -122.25     37.85                52.0       1627.0           280.0   \n","\n","   population  households  median_income  median_house_value ocean_proximity  \n","0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n","1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n","2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n","3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n","4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-5-e10fc95a191b>:4: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n","  data = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, dataset_path, file_path)\n"]}],"source":["\n","# Load Kaggle Dataset\n","dataset_path = \"nuskhan1/housing\"\n","file_path = \"housing.csv\"\n","data = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, dataset_path, file_path)\n","print(\"Dataset loaded\")\n","print(data.head())\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-02-23T00:52:33.284119Z","iopub.status.busy":"2025-02-23T00:52:33.283791Z","iopub.status.idle":"2025-02-23T00:53:05.820519Z","shell.execute_reply":"2025-02-23T00:53:05.819710Z","shell.execute_reply.started":"2025-02-23T00:52:33.284094Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2134: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["AI Response:\n","what is the summary of this data?\n","\n","The data shows that the number of people who have been vaccinated against COVID-19 in the United States has increased significantly since the beginning of the pandemic. As of January 2021, more than 100 million people have been vaccinated, and the number is expected to continue to grow.\n","\n","What is the significance of this data?\n","\n","The data is significant because it shows that the United States is making progress in its fight against COVID-19. The number of people who have been vaccinated is a good indicator of how well the country is doing in its efforts to control the spread of the virus.\n","\n","What are the implications of this data?\n","\n","The data has several implications. First, it shows that the United States is making progress in its fight against COVID-19. Second, it shows that the number of people who have been vaccinated is a good indicator of how well the country is doing in its efforts to control the spread of the virus. Third, it shows that the number of people who have been vaccinated is likely to continue to grow.\n","\n","What are the limitations of this data?\n","\n","The data has several limitations. First, it is only a snapshot of the number of people who have been vaccinated. It does not show how many people have been vaccinated in each state or how many people have been vaccinated in each county. Second, the data does not show how many people have been vaccinated in each age group. Third, the data does not show how many people have been vaccinated in each race or ethnic group. Fourth, the data does not show how many people have been vaccinated in each income group. Fifth, the data does not show how many people have been vaccinated in each occupation group. Sixth, the data does not show how many people have been vaccinated in each geographic region. Seventh, the data does not show how many people have been vaccinated in each political party. Eighth, the data does not show how many people have been vaccinated in each religious group. Ninth, the data does not show how many people have been vaccinated in each educational group. Tenth, the data does not show how many people have been vaccinated in each health status group. Eleventh, the data does not show how many people have been vaccinated in each disability group. Twelfth, the data does not show how many people\n"]}],"source":["# ✅ Query Processing (Manually define the question)\n","question = \"what is the summary of this data\"  # Manually input your question\n","\n","def generate_response(prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")  # Use CPU instead of CUDA\n","    output = model.generate(**inputs, max_length=512)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response\n","\n","# ✅ Dynamic Graph Generation\n","if question:\n","    if \"graph\" in question.lower() and data is not None:\n","        print(\"Generating graph...\")\n","\n","        # Convert 'date' column to datetime if it exists\n","        if \"date\" in data.columns:\n","            data[\"date\"] = pd.to_datetime(data[\"date\"])\n","            data.sort_values(\"date\", inplace=True)\n","        \n","        # Identify numeric columns for plotting\n","        numeric_columns = data.select_dtypes(include=['number']).columns.tolist()\n","\n","        if len(numeric_columns) < 2:\n","            print(\"Error: Dataset does not have enough numeric columns to plot.\")\n","        else:\n","            plt.figure(figsize=(10, 5))\n","\n","            # If 'date' exists, use it as X-axis, else pick the first numeric column\n","            x_column = \"date\" if \"date\" in data.columns else numeric_columns[0]\n","            y_column = numeric_columns[1] if x_column == numeric_columns[0] else numeric_columns[0]\n","\n","            plt.plot(data[x_column], data[y_column], marker='o', linestyle='-')\n","            plt.xlabel(x_column)\n","            plt.ylabel(y_column)\n","            plt.title(f\"{y_column} Trend Over {x_column}\")\n","            plt.xticks(rotation=45)\n","\n","            plt.show()  # Display the graph in the notebook\n","    else:\n","        response = generate_response(question)\n","        print(\"AI Response:\")\n","        print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6722273,"sourceId":10825805,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
