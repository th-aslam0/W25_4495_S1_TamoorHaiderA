{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#%%capture\n#! pip install unsloth\n#! pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:31.260591Z","iopub.execute_input":"2025-02-17T07:34:31.260991Z","iopub.status.idle":"2025-02-17T07:34:45.417558Z","shell.execute_reply.started":"2025-02-17T07:34:31.260962Z","shell.execute_reply":"2025-02-17T07:34:45.416659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install adapter-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.419002Z","iopub.execute_input":"2025-02-17T07:34:45.419255Z","iopub.status.idle":"2025-02-17T07:34:45.424303Z","shell.execute_reply.started":"2025-02-17T07:34:45.419233Z","shell.execute_reply":"2025-02-17T07:34:45.423516Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ“ Full Kaggle Notebook Code for Fine-Tuning DeepSeek R1          \nI'll structure it in steps:                        \n                         \n1ï¸âƒ£ Setup & Install Dependencies                \n2ï¸âƒ£ Authenticate with Hugging Face & W&B                  \n3ï¸âƒ£ Load DeepSeek R1 & Tokenizer                               \n4ï¸âƒ£ Load Three Hugging Face Datasets                                \n5ï¸âƒ£ Preprocess Data for Fine-Tuning                                       \n6ï¸âƒ£ Fine-Tune the Model                                           \n7ï¸âƒ£ Save & Download Fine-Tuned Model               ","metadata":{}},{"cell_type":"code","source":"#!pip install datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.425927Z","iopub.execute_input":"2025-02-17T07:34:45.426224Z","iopub.status.idle":"2025-02-17T07:34:45.446596Z","shell.execute_reply.started":"2025-02-17T07:34:45.426189Z","shell.execute_reply":"2025-02-17T07:34:45.445789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from datasets import load_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.447934Z","iopub.execute_input":"2025-02-17T07:34:45.448170Z","iopub.status.idle":"2025-02-17T07:34:45.462303Z","shell.execute_reply.started":"2025-02-17T07:34:45.448150Z","shell.execute_reply":"2025-02-17T07:34:45.461682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from kaggle_secrets import UserSecretsClient\n#user_secrets = UserSecretsClient()\n#secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n#secret_value_1 = user_secrets.get_secret(\"wnb \")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.463070Z","iopub.execute_input":"2025-02-17T07:34:45.463376Z","iopub.status.idle":"2025-02-17T07:34:45.480864Z","shell.execute_reply.started":"2025-02-17T07:34:45.463354Z","shell.execute_reply":"2025-02-17T07:34:45.480153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### import all relevant packages","metadata":{}},{"cell_type":"code","source":"#Modules for fine tuning\nfrom unsloth import FastLanguageModel\nimport torch\nfrom trl import SFTTrainer # trainer for supervised fine tuning\nfrom unsloth import is_bf16_supported # checks if the hardware supports bfloat16 presion\n\n#hugging face modlue\nfrom huggingface_hub import login #lets you to login to api\nfrom transformers import TrainingArguments #defines training hyperparameters\n#from dataset import load_dataset #lets you to load fine tuning datasets\n#import weights and biases\nimport wandb\n#import keggle secrets\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset, concatenate_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.481558Z","iopub.execute_input":"2025-02-17T07:34:45.481769Z","iopub.status.idle":"2025-02-17T07:34:45.499697Z","shell.execute_reply.started":"2025-02-17T07:34:45.481750Z","shell.execute_reply":"2025-02-17T07:34:45.498796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#! pip uninstall torch torchvision torchaudio\n#! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.500608Z","iopub.execute_input":"2025-02-17T07:34:45.500912Z","iopub.status.idle":"2025-02-17T07:34:45.515684Z","shell.execute_reply.started":"2025-02-17T07:34:45.500884Z","shell.execute_reply":"2025-02-17T07:34:45.515086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### create API keys and login to Hugging face and wights and biases","metadata":{}},{"cell_type":"code","source":"#initializing hugging face and w&b tokens\nuser_secrets=UserSecretsClient() #from keggle secrets import UserSecretClient\nhugging_face_token=user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token=user_secrets.get_secret(\"wnb\")\n\n#from kaggle_secrets import UserSecretsClient\n#user_secrets = UserSecretsClient()\n#secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n#secret_value_1 = user_secrets.get_secret(\"wnb \")\n\n\n#login to hugging face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n#login to wnb\nwandb.login(key=wnb_token) #import wandb\nrun=wandb.init(\n    project='deepseek',\n    job_type='training',\n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.517752Z","iopub.execute_input":"2025-02-17T07:34:45.517939Z","iopub.status.idle":"2025-02-17T07:34:45.920964Z","shell.execute_reply.started":"2025-02-17T07:34:45.517923Z","shell.execute_reply":"2025-02-17T07:34:45.920288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### loading Deepseek r1 and the tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n#set parameters\nmax_seq_length=2048 #defining the maximum sequence length a model can handle \ndtype=None # default data type (usually auto-detected)\nload_in_4bit=True #enables 4-bit quantization- a memory saving optimization\n#load the deepseek r1 and tokenizer using unsloth\nmodel,tokenizer=FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=hugging_face_token,\n    device_map=\"auto\",  # This tries to intelligently place the model on available devices (GPU, CPU)\n    #llm_int8_enable_fp32_cpu_offload=True,  # Enable offloading to CPU\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:45.922416Z","iopub.execute_input":"2025-02-17T07:34:45.922630Z","iopub.status.idle":"2025-02-17T07:34:48.124581Z","shell.execute_reply.started":"2025-02-17T07:34:45.922610Z","shell.execute_reply":"2025-02-17T07:34:48.123303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###Â  Load Three Hugging Face Datasets","metadata":{}},{"cell_type":"code","source":"\n\n# Load datasets\ndataset_1 = load_dataset(\"OpenAssistant/oasst1\", split=\"train[:10%]\")  # Example: Chatbot fine-tuning\ndataset_2 = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")  # Instruction-based dataset\ndataset_3 = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")  # LLaMA-2 fine-tuning dataset\n\n# Check dataset sizes\nprint(\"Dataset 1 size:\", len(dataset_1))\nprint(\"Dataset 2 size:\", len(dataset_2))\nprint(\"Dataset 3 size:\", len(dataset_3))\n\n# Limit the number of samples based on the dataset size\ndataset_1 = dataset_1.shuffle().select(range(min(5000, len(dataset_1))))\ndataset_2 = dataset_2.shuffle().select(range(min(5000, len(dataset_2))))\ndataset_3 = dataset_3.shuffle().select(range(min(5000, len(dataset_3))))\n\n# Concatenate datasets using concatenate_datasets()\ncombined_datasets = concatenate_datasets([dataset_1, dataset_2, dataset_3])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.125202Z","iopub.status.idle":"2025-02-17T07:34:48.125486Z","shell.execute_reply":"2025-02-17T07:34:48.125367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the columns of the datasets\nprint(dataset_1.column_names)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.126442Z","iopub.status.idle":"2025-02-17T07:34:48.126911Z","shell.execute_reply":"2025-02-17T07:34:48.126720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dataset_1","metadata":{}},{"cell_type":"code","source":"print(dataset_2.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.127610Z","iopub.status.idle":"2025-02-17T07:34:48.127956Z","shell.execute_reply":"2025-02-17T07:34:48.127837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(dataset_3.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.128929Z","iopub.status.idle":"2025-02-17T07:34:48.129233Z","shell.execute_reply":"2025-02-17T07:34:48.129097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess Data for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Preprocess function to handle the tokenization based on the column structure\ndef preprocess_function(examples, dataset_name):\n    # For dataset_1, use the 'text' column\n    if dataset_name == \"dataset_1\":\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n    # For dataset_2, concatenate 'instruction', 'context', and 'response' as one string\n    elif dataset_name == \"dataset_2\":\n        combined_text = [f\"Instruction: {x} \\nContext: {y} \\nResponse: {z}\" \n                         for x, y, z in zip(examples[\"instruction\"], examples[\"context\"], examples[\"response\"])]\n        return tokenizer(combined_text, truncation=True, padding=\"max_length\", max_length=2048)\n    # For dataset_3, use the 'text' column\n    elif dataset_name == \"dataset_3\":\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n\n# Apply preprocessing function to each dataset\ntokenized_dataset_1 = dataset_1.map(lambda examples: preprocess_function(examples, \"dataset_1\"), batched=True)\ntokenized_dataset_2 = dataset_2.map(lambda examples: preprocess_function(examples, \"dataset_2\"), batched=True)\ntokenized_dataset_3 = dataset_3.map(lambda examples: preprocess_function(examples, \"dataset_3\"), batched=True)\n\n# Combine the datasets into one\nfrom datasets import concatenate_datasets\ncombined_datasets = concatenate_datasets([tokenized_dataset_1, tokenized_dataset_2, tokenized_dataset_3])\n\n# Split the combined dataset into train and test\ntrain_test_split = combined_datasets.train_test_split(test_size=0.1)\n\n# Access the training and testing datasets\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Optionally, print the first few examples after tokenization\nprint(\"Train dataset example:\", train_dataset[:2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.130003Z","iopub.status.idle":"2025-02-17T07:34:48.130333Z","shell.execute_reply":"2025-02-17T07:34:48.130205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers>=4.8\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.131061Z","iopub.status.idle":"2025-02-17T07:34:48.131397Z","shell.execute_reply":"2025-02-17T07:34:48.131265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install adapter-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.132135Z","iopub.status.idle":"2025-02-17T07:34:48.132526Z","shell.execute_reply":"2025-02-17T07:34:48.132367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install adapter-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.133329Z","iopub.status.idle":"2025-02-17T07:34:48.133710Z","shell.execute_reply":"2025-02-17T07:34:48.133551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the column names of the dataset\n#print(train_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.134317Z","iopub.status.idle":"2025-02-17T07:34:48.134657Z","shell.execute_reply":"2025-02-17T07:34:48.134537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from adapter_transformers import AdapterConfig, AdapterTrainer\nfrom transformers import Trainer, TrainingArguments  # Keep this for the Trainer and TrainingArguments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.135305Z","iopub.status.idle":"2025-02-17T07:34:48.135720Z","shell.execute_reply":"2025-02-17T07:34:48.135534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fine-Tune the Model ","metadata":{}},{"cell_type":"code","source":"from transformers import AdapterConfig, Trainer, TrainingArguments\nfrom unsloth import FastLanguageModel\nfrom adapter_transformers import AdapterTrainer, AdapterConfig\n\n# Step 1: Load Quantized Model and Add Adapters\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n    token=hugging_face_token\n)\n\n# Add an adapter\nadapter_name = \"adapter_1\"\nmodel.add_adapter(adapter_name, config=AdapterConfig.load(\"pfeiffer\"))\n\n# Step 2: Enable the adapter for training\nmodel.set_active_adapters(adapter_name)\n\n# Step 3: Preprocess Dataset 1 (same as before)\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n\ntokenized_dataset_1 = dataset_1.map(preprocess_function, batched=True)\n\n# Split the dataset into train and test sets\ntrain_test_split = tokenized_dataset_1.train_test_split(test_size=0.1)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Step 4: Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_steps=1000,\n    save_total_limit=3,\n)\n\n# Step 5: Initialize the Trainer with AdapterTrainer\ntrainer = AdapterTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n)\n\n# Step 6: Start Training\ntrainer.train()\n\n# Step 7: Evaluate the Model\ntrainer.evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.136827Z","iopub.status.idle":"2025-02-17T07:34:48.137085Z","shell.execute_reply":"2025-02-17T07:34:48.136977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./deepseek-finetuned\",  # Output directory\n    per_device_train_batch_size=2,  # Adjust batch size based on memory\n    per_device_eval_batch_size=2,  # Evaluation batch size\n    learning_rate=2e-5,  # Learning rate for fine-tuning\n    weight_decay=0.01,  # Weight decay for regularization\n    num_train_epochs=3,  # Number of epochs for training\n    logging_dir=\"./logs\",  # Logging directory\n    logging_steps=10,  # Log every 10 steps\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",  # Save model at the end of each epoch\n    report_to=\"wandb\"  # Log metrics to W&B\n)\n\n# Ensure you specify the correct text field\n# Ensure you specify the correct text field\ntrain_dataset = train_dataset.map(lambda ex: {'text': ex['text']})\neval_dataset = eval_dataset.map(lambda ex: {'text': ex['text']})\n\n\n# Now when passing to the trainer, the correct field will be used\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=training_args,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\"  # Specify the correct text field here\n)\n# Start fine-tuning\ntrainer.train()\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:34:48.137734Z","iopub.status.idle":"2025-02-17T07:34:48.138088Z","shell.execute_reply":"2025-02-17T07:34:48.137946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}